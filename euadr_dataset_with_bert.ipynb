{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "euadr-dataset-with-bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMG7fjY/fnASymVr68Pvem0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hushee69/biobert-relation-extraction/blob/main/euadr_dataset_with_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asONPSIFkEx3",
        "outputId": "871885b4-1f60-4498-81eb-60ebc415c82b"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoWs_ms5XRw-"
      },
      "source": [
        "import numpy as np;\r\n",
        "import pandas as pd;\r\n",
        "\r\n",
        "import torch;\r\n",
        "\r\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig;\r\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler;\r\n",
        "from transformers import get_linear_schedule_with_warmup;\r\n",
        "\r\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score;"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJEUllxpY7dM",
        "outputId": "722b7258-d70a-4889-9722-fd894fefb663"
      },
      "source": [
        "from google.colab import drive;\r\n",
        "\r\n",
        "drive.mount('/content/gdrive');"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcYLTaLhdGJ"
      },
      "source": [
        "path='/content/gdrive/MyDrive/biobert_re/';"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6iKt23StQFZ"
      },
      "source": [
        "device = torch.device('cuda');"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rH4qL4Cl_Mi"
      },
      "source": [
        "def accuracy(preds, labels):\r\n",
        "    preds = np.argmax(preds, axis=1).flatten();\r\n",
        "    labels = labels.flatten();\r\n",
        "\r\n",
        "    return np.sum(preds == labels) / len(labels);"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiR5EcY7YwkG"
      },
      "source": [
        "\"\"\"\r\n",
        "    Params:\r\n",
        "        filepath: path of the dataset\r\n",
        "        tokenizer: tokenizer to use\r\n",
        "        maxlen: maxlength of text\r\n",
        "\"\"\"\r\n",
        "def load_and_process_euadr_train_data(filepath, tokenizer, maxlen=512, train_percentage=0.7):\r\n",
        "    # load dataset\r\n",
        "    df = pd.read_csv(filepath, header=None, delimiter='\\t', names=['sentence', 'label']);\r\n",
        "\r\n",
        "    sentences = df.sentence.values;\r\n",
        "    labels = df.label.values;\r\n",
        "    \r\n",
        "    input_ids = [];\r\n",
        "    attention_masks = [];\r\n",
        "    \r\n",
        "    for sent in sentences:\r\n",
        "        encoded_dict = tokenizer.encode_plus(\r\n",
        "            sent,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=maxlen,\r\n",
        "            padding='max_length',\r\n",
        "            truncation=True,\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt'\r\n",
        "        );\r\n",
        "\r\n",
        "        input_ids.append(encoded_dict['input_ids']);\r\n",
        "        attention_masks.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "    # convert lists into tensors\r\n",
        "    input_ids = torch.cat(input_ids, dim=0);\r\n",
        "    attention_masks = torch.cat(attention_masks, dim=0);\r\n",
        "    labels_tensor = torch.tensor(labels);\r\n",
        "\r\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels_tensor);\r\n",
        "\r\n",
        "    train_size = int(train_percentage * len(dataset));\r\n",
        "    val_size = len(dataset) - train_size;\r\n",
        "\r\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size]);\r\n",
        "\r\n",
        "    return (train_dataset, val_dataset);"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5hxq5PBk3w_"
      },
      "source": [
        "def train_euadr_model(model, data, optimizer=AdamW, batch_size=32, epochs=3):\r\n",
        "    max_val_loss = np.float('inf');\r\n",
        "\r\n",
        "    train_ds = data[0];\r\n",
        "    val_ds = data[1];\r\n",
        "\r\n",
        "    train_dataloader = DataLoader(\r\n",
        "        train_ds,\r\n",
        "        sampler=RandomSampler(train_ds),\r\n",
        "        batch_size=batch_size\r\n",
        "    );\r\n",
        "\r\n",
        "    val_dataloader = DataLoader(\r\n",
        "        val_ds,\r\n",
        "        sampler=SequentialSampler(val_ds),\r\n",
        "        batch_size=batch_size\r\n",
        "    );\r\n",
        "\r\n",
        "    for e in range(epochs):\r\n",
        "        train_loss = 0;\r\n",
        "        train_acc = 0;\r\n",
        "\r\n",
        "        model.train();\r\n",
        "\r\n",
        "        optim = optimizer(model.parameters(), lr=2e-5, eps=1e-8);\r\n",
        "\r\n",
        "        scheduler = get_linear_schedule_with_warmup(\r\n",
        "            optim,\r\n",
        "            num_warmup_steps=0,\r\n",
        "            num_training_steps=len(train_dataloader) * epochs\r\n",
        "        );\r\n",
        "\r\n",
        "        for batch in train_dataloader:\r\n",
        "            b_input_ids = batch[0].to(device);\r\n",
        "            b_input_mask = batch[1].to(device);\r\n",
        "            b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "            model.zero_grad();\r\n",
        "\r\n",
        "            output = model(\r\n",
        "                b_input_ids,\r\n",
        "                token_type_ids=None,\r\n",
        "                attention_mask=b_input_mask,\r\n",
        "                labels=b_labels\r\n",
        "            );\r\n",
        "\r\n",
        "            loss = output['loss'];\r\n",
        "            preds = output['logits'].detach().cpu().numpy();\r\n",
        "            labels = b_labels.to('cpu').numpy();\r\n",
        "\r\n",
        "            train_loss += loss.item();\r\n",
        "            train_acc += accuracy(preds, labels);\r\n",
        "\r\n",
        "            loss.backward();\r\n",
        "\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0);\r\n",
        "\r\n",
        "            optim.step();\r\n",
        "\r\n",
        "            scheduler.step();\r\n",
        "        \r\n",
        "        avg_train_loss = train_loss / len(train_dataloader);\r\n",
        "        avg_train_acc = train_acc / len(train_dataloader);\r\n",
        "\r\n",
        "        print('average training loss for epoch: {}'.format(avg_train_loss));\r\n",
        "        print('average training accuracy for epoch: {}'.format(avg_train_acc));\r\n",
        "\r\n",
        "        # validation\r\n",
        "        val_loss = 0;\r\n",
        "        val_acc = 0;\r\n",
        "\r\n",
        "        model.eval();\r\n",
        "\r\n",
        "        for batch in val_dataloader:\r\n",
        "            b_input_ids = batch[0].to(device);\r\n",
        "            b_attention_mask = batch[1].to(device);\r\n",
        "            b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "            with torch.no_grad():\r\n",
        "                output = model(\r\n",
        "                    b_input_ids,\r\n",
        "                    token_type_ids=None,\r\n",
        "                    attention_mask=b_attention_mask,\r\n",
        "                    labels=b_labels\r\n",
        "                );\r\n",
        "            \r\n",
        "            loss = output['loss'];\r\n",
        "            preds = output['logits'].detach().cpu().numpy();\r\n",
        "            labels = b_labels.to('cpu').numpy();\r\n",
        "\r\n",
        "            val_loss += loss.item();\r\n",
        "            val_acc += accuracy(preds, labels);\r\n",
        "        \r\n",
        "        avg_val_loss = val_loss / len(val_dataloader);\r\n",
        "        avg_val_acc = val_acc / len(val_dataloader);\r\n",
        "\r\n",
        "        if avg_val_loss < max_val_loss:\r\n",
        "            max_val_loss = avg_val_loss;\r\n",
        "            torch.save(model.state_dict(), 'best_model.pt');\r\n",
        "\r\n",
        "        print('average validation loss for epoch: {}'.format(avg_val_loss));\r\n",
        "        print('average validation accuracy for epoch: {}'.format(avg_val_acc));"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYz30W6M16nb"
      },
      "source": [
        "**Training EU-ADR**\r\n",
        "\r\n",
        "Since there is not much data in EU-ADR dataset, we chose to have 3 epochs so as to prevent overfitting of data. This decision was not experimental since the authors advise fine-tuning using 2-4 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N2QWcTMkBT9",
        "outputId": "c48814ab-beee-48ca-a08f-b53d1957acff"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased');\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained(\r\n",
        "    'bert-base-uncased',\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions=False,\r\n",
        "    output_hidden_states=False\r\n",
        ");\r\n",
        "\r\n",
        "device = torch.device('cuda');\r\n",
        "model.cuda();\r\n",
        "\r\n",
        "epochs = 2;"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nPNQdAJi1Ff",
        "outputId": "3883c0ee-66d5-4d59-ec86-30d099874e95"
      },
      "source": [
        "for i in range(10):\r\n",
        "    data = load_and_process_euadr_train_data(path + 'euadr/' + str(i + 1) + '/train.tsv', tokenizer=tokenizer, maxlen=128);\r\n",
        "    train_euadr_model(model, data);"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training loss for epoch: 0.5665075821535928\n",
            "average training accuracy for epoch: 0.7568452380952381\n",
            "average validation loss for epoch: 0.6218223869800568\n",
            "average validation accuracy for epoch: 0.6979166666666666\n",
            "average training loss for epoch: 0.5409264692238399\n",
            "average training accuracy for epoch: 0.7577380952380953\n",
            "average validation loss for epoch: 0.5830157597859701\n",
            "average validation accuracy for epoch: 0.71875\n",
            "average training loss for epoch: 0.5119139424392155\n",
            "average training accuracy for epoch: 0.7526785714285714\n",
            "average validation loss for epoch: 0.5833379228909811\n",
            "average validation accuracy for epoch: 0.7083333333333334\n",
            "average training loss for epoch: 0.501392798764365\n",
            "average training accuracy for epoch: 0.7744047619047619\n",
            "average validation loss for epoch: 0.5318445563316345\n",
            "average validation accuracy for epoch: 0.7291666666666666\n",
            "average training loss for epoch: 0.43372476526669096\n",
            "average training accuracy for epoch: 0.8205357142857144\n",
            "average validation loss for epoch: 0.597226748863856\n",
            "average validation accuracy for epoch: 0.75\n",
            "average training loss for epoch: 0.4214684580053602\n",
            "average training accuracy for epoch: 0.8199404761904762\n",
            "average validation loss for epoch: 0.5483659505844116\n",
            "average validation accuracy for epoch: 0.75\n",
            "average training loss for epoch: 0.4591009191104344\n",
            "average training accuracy for epoch: 0.8165322580645161\n",
            "average validation loss for epoch: 0.3886124591032664\n",
            "average validation accuracy for epoch: 0.8125\n",
            "average training loss for epoch: 0.428911702973502\n",
            "average training accuracy for epoch: 0.8339573732718895\n",
            "average validation loss for epoch: 0.3857078750928243\n",
            "average validation accuracy for epoch: 0.84375\n",
            "average training loss for epoch: 0.3920703019414629\n",
            "average training accuracy for epoch: 0.8518145161290323\n",
            "average validation loss for epoch: 0.4087120195229848\n",
            "average validation accuracy for epoch: 0.8333333333333334\n",
            "average training loss for epoch: 0.4276187888213566\n",
            "average training accuracy for epoch: 0.8125\n",
            "average validation loss for epoch: 0.3672880530357361\n",
            "average validation accuracy for epoch: 0.8333333333333334\n",
            "average training loss for epoch: 0.38661201085363117\n",
            "average training accuracy for epoch: 0.8482142857142857\n",
            "average validation loss for epoch: 0.4326619903246562\n",
            "average validation accuracy for epoch: 0.8125\n",
            "average training loss for epoch: 0.34892160977636066\n",
            "average training accuracy for epoch: 0.875\n",
            "average validation loss for epoch: 0.33772961298624676\n",
            "average validation accuracy for epoch: 0.8333333333333334\n",
            "average training loss for epoch: 0.29657821357250214\n",
            "average training accuracy for epoch: 0.8928571428571429\n",
            "average validation loss for epoch: 0.3664098580678304\n",
            "average validation accuracy for epoch: 0.84375\n",
            "average training loss for epoch: 0.2749102009194238\n",
            "average training accuracy for epoch: 0.9151785714285714\n",
            "average validation loss for epoch: 0.42256611585617065\n",
            "average validation accuracy for epoch: 0.8229166666666666\n",
            "average training loss for epoch: 0.23134484461375646\n",
            "average training accuracy for epoch: 0.9151785714285714\n",
            "average validation loss for epoch: 0.43654292821884155\n",
            "average validation accuracy for epoch: 0.8020833333333334\n",
            "average training loss for epoch: 0.38802656531333923\n",
            "average training accuracy for epoch: 0.84375\n",
            "average validation loss for epoch: 0.2717800090710322\n",
            "average validation accuracy for epoch: 0.90625\n",
            "average training loss for epoch: 0.3099952978747232\n",
            "average training accuracy for epoch: 0.875\n",
            "average validation loss for epoch: 0.22122389574845633\n",
            "average validation accuracy for epoch: 0.9270833333333334\n",
            "average training loss for epoch: 0.26018182933330536\n",
            "average training accuracy for epoch: 0.8928571428571429\n",
            "average validation loss for epoch: 0.24750515321890512\n",
            "average validation accuracy for epoch: 0.8958333333333334\n",
            "average training loss for epoch: 0.23977258588586534\n",
            "average training accuracy for epoch: 0.8973214285714286\n",
            "average validation loss for epoch: 0.262099285920461\n",
            "average validation accuracy for epoch: 0.8958333333333334\n",
            "average training loss for epoch: 0.1975949832371303\n",
            "average training accuracy for epoch: 0.9107142857142857\n",
            "average validation loss for epoch: 0.44793041547139484\n",
            "average validation accuracy for epoch: 0.84375\n",
            "average training loss for epoch: 0.17708697063582285\n",
            "average training accuracy for epoch: 0.9241071428571429\n",
            "average validation loss for epoch: 0.3540559411048889\n",
            "average validation accuracy for epoch: 0.84375\n",
            "average training loss for epoch: 0.18637542213712419\n",
            "average training accuracy for epoch: 0.9196428571428571\n",
            "average validation loss for epoch: 0.1427036573489507\n",
            "average validation accuracy for epoch: 0.9479166666666666\n",
            "average training loss for epoch: 0.1702827662229538\n",
            "average training accuracy for epoch: 0.9375\n",
            "average validation loss for epoch: 0.11694341401259105\n",
            "average validation accuracy for epoch: 0.9479166666666666\n",
            "average training loss for epoch: 0.118135538484369\n",
            "average training accuracy for epoch: 0.9419642857142857\n",
            "average validation loss for epoch: 0.17263908932606378\n",
            "average validation accuracy for epoch: 0.9270833333333334\n",
            "average training loss for epoch: 0.16025111185652868\n",
            "average training accuracy for epoch: 0.9464285714285714\n",
            "average validation loss for epoch: 0.14625434825817743\n",
            "average validation accuracy for epoch: 0.9583333333333334\n",
            "average training loss for epoch: 0.14866532598223006\n",
            "average training accuracy for epoch: 0.9375\n",
            "average validation loss for epoch: 0.22425885498523712\n",
            "average validation accuracy for epoch: 0.9375\n",
            "average training loss for epoch: 0.13453217595815659\n",
            "average training accuracy for epoch: 0.9419642857142857\n",
            "average validation loss for epoch: 0.1589034547408422\n",
            "average validation accuracy for epoch: 0.9479166666666666\n",
            "average training loss for epoch: 0.1993284672498703\n",
            "average training accuracy for epoch: 0.9196428571428571\n",
            "average validation loss for epoch: 0.13538884619871774\n",
            "average validation accuracy for epoch: 0.9375\n",
            "average training loss for epoch: 0.11883034610322543\n",
            "average training accuracy for epoch: 0.9553571428571429\n",
            "average validation loss for epoch: 0.1636108160018921\n",
            "average validation accuracy for epoch: 0.9375\n",
            "average training loss for epoch: 0.0876595356634685\n",
            "average training accuracy for epoch: 0.9732142857142857\n",
            "average validation loss for epoch: 0.17068792258699736\n",
            "average validation accuracy for epoch: 0.9479166666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WTXMD-l21ko"
      },
      "source": [
        "from IPython.display import display;"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl9LBgJ92Wlq"
      },
      "source": [
        "\"\"\"\r\n",
        "    Params:\r\n",
        "        filepath: path of the dataset\r\n",
        "        tokenizer: tokenizer to use\r\n",
        "        maxlen: maxlength of text\r\n",
        "\"\"\"\r\n",
        "def load_and_process_euadr_test_data(filepath, tokenizer, maxlen=512):\r\n",
        "    # load dataset\r\n",
        "    df = pd.read_csv(filepath, delimiter='\\t', index_col=0);\r\n",
        "\r\n",
        "    sentences = df.sentence.values;\r\n",
        "    labels = df.label.values;\r\n",
        "    \r\n",
        "    input_ids = [];\r\n",
        "    attention_masks = [];\r\n",
        "    \r\n",
        "    for sent in sentences:\r\n",
        "        encoded_dict = tokenizer.encode_plus(\r\n",
        "            sent,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=maxlen,\r\n",
        "            padding='max_length',\r\n",
        "            truncation=True,\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt'\r\n",
        "        );\r\n",
        "\r\n",
        "        input_ids.append(encoded_dict['input_ids']);\r\n",
        "        attention_masks.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "    # convert lists into tensors\r\n",
        "    input_ids = torch.cat(input_ids, dim=0);\r\n",
        "    attention_masks = torch.cat(attention_masks, dim=0);\r\n",
        "    labels_tensor = torch.tensor(labels);\r\n",
        "\r\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels_tensor);\r\n",
        "\r\n",
        "    return dataset;"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no5-wVSo5_ZV"
      },
      "source": [
        "def test_euadr_model(model, data, batch_size=32):\r\n",
        "    model.load_state_dict(torch.load('best_model.pt'));\r\n",
        "\r\n",
        "    model.eval();\r\n",
        "\r\n",
        "    ds = data;\r\n",
        "\r\n",
        "    test_dataloader = DataLoader(\r\n",
        "        ds,\r\n",
        "        sampler=SequentialSampler(ds),\r\n",
        "        batch_size=batch_size,\r\n",
        "    );\r\n",
        "\r\n",
        "    preds, real_labels = [], [];\r\n",
        "\r\n",
        "    total_accuracy_for_file = 0;\r\n",
        "    total_precision_for_file = 0;\r\n",
        "    total_recall_for_file = 0;\r\n",
        "    total_f1_score_for_file = 0;\r\n",
        "\r\n",
        "    for batch_nbr, batch in enumerate(test_dataloader):\r\n",
        "        b_input_ids = batch[0].to(device);\r\n",
        "        b_input_mask = batch[1].to(device);\r\n",
        "        b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            outputs = model(\r\n",
        "                b_input_ids,\r\n",
        "                token_type_ids=None,\r\n",
        "                attention_mask=b_input_mask\r\n",
        "            );\r\n",
        "        \r\n",
        "        logits = outputs['logits'].detach().cpu().numpy();\r\n",
        "        label_ids = b_labels.to('cpu').numpy();\r\n",
        "        preds = np.argmax(logits, axis=1);\r\n",
        "        \r\n",
        "        batch_acc = accuracy(logits, label_ids);\r\n",
        "        total_accuracy_for_file += batch_acc;\r\n",
        "        \r\n",
        "        batch_precision = precision_score(label_ids, preds);\r\n",
        "        total_precision_for_file += batch_precision;\r\n",
        "\r\n",
        "        batch_recall = recall_score(label_ids, preds);\r\n",
        "        total_recall_for_file += batch_recall;\r\n",
        "\r\n",
        "        batch_f1 = f1_score(label_ids, preds);\r\n",
        "        total_f1_score_for_file += batch_f1\r\n",
        "\r\n",
        "        print('accuracy for batch {}: {}'.format(batch_nbr, batch_acc));\r\n",
        "        print('precision for batch{}: {}'.format(batch_nbr, batch_precision));\r\n",
        "        print('recall for batch{}: {}'.format(batch_nbr, batch_recall));\r\n",
        "        print('f1 score for batch{}: {}'.format(batch_nbr, batch_f1));\r\n",
        "    \r\n",
        "    batch_nbr += 1;\r\n",
        "    ret = (\r\n",
        "        total_accuracy_for_file / batch_nbr,\r\n",
        "        total_precision_for_file / batch_nbr,\r\n",
        "        total_recall_for_file / batch_nbr,\r\n",
        "        total_f1_score_for_file / batch_nbr\r\n",
        "    );\r\n",
        "    \r\n",
        "    return ret;"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKkzrVTw7dQK",
        "outputId": "966c1210-fd14-4ca2-a864-19ee80efc4ca"
      },
      "source": [
        "total_accuracy_for_files = 0;\r\n",
        "total_precision_for_files = 0;\r\n",
        "total_recall_for_files = 0;\r\n",
        "total_f1_score_for_files = 0;\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    data = load_and_process_euadr_test_data(path + 'euadr/' + str(i + 1) + '/test.tsv', tokenizer=tokenizer, maxlen=128);\r\n",
        "    ret = test_euadr_model(model, data);\r\n",
        "    total_accuracy_for_files += ret[0];\r\n",
        "    total_precision_for_files += ret[1];\r\n",
        "    total_recall_for_files += ret[2];\r\n",
        "    total_f1_score_for_files += ret[3];\r\n",
        "\r\n",
        "    print();\r\n",
        "    print('accuracy for file {}: {}'.format((i + 1), ret[0]));\r\n",
        "    print('precision for file {}: {}'.format((i + 1), ret[1]));\r\n",
        "    print('recall for file {}: {}'.format((i + 1), ret[2]));\r\n",
        "    print('f1 score for file {}: {}'.format((i + 1), ret[3]));\r\n",
        "    print();\r\n",
        "    print();\r\n",
        "\r\n",
        "# 10 files\r\n",
        "print('test accuracy for all files: {}%'.format(((total_accuracy_for_files) / 10) * 100));\r\n",
        "print('test precision for all files: {}%'.format(((total_precision_for_files) / 10) * 100));\r\n",
        "print('test recall for all files: {}%'.format(((total_recall_for_files) / 10) * 100));\r\n",
        "print('test f1 score for all files: {}%'.format(((total_f1_score_for_files) / 10) * 100));"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy for batch 0: 1.0\n",
            "precision for batch0: 1.0\n",
            "recall for batch0: 1.0\n",
            "f1 score for batch0: 1.0\n",
            "accuracy for batch 1: 0.8\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 0.8\n",
            "f1 score for batch1: 0.888888888888889\n",
            "\n",
            "accuracy for file 1: 0.9\n",
            "precision for file 1: 1.0\n",
            "recall for file 1: 0.9\n",
            "f1 score for file 1: 0.9444444444444444\n",
            "\n",
            "\n",
            "accuracy for batch 0: 0.96875\n",
            "precision for batch0: 1.0\n",
            "recall for batch0: 0.9545454545454546\n",
            "f1 score for batch0: 0.9767441860465117\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 2: 0.984375\n",
            "precision for file 2: 1.0\n",
            "recall for file 2: 0.9772727272727273\n",
            "f1 score for file 2: 0.9883720930232558\n",
            "\n",
            "\n",
            "accuracy for batch 0: 0.96875\n",
            "precision for batch0: 0.9615384615384616\n",
            "recall for batch0: 1.0\n",
            "f1 score for batch0: 0.9803921568627451\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 3: 0.984375\n",
            "precision for file 3: 0.9807692307692308\n",
            "recall for file 3: 1.0\n",
            "f1 score for file 3: 0.9901960784313726\n",
            "\n",
            "\n",
            "accuracy for batch 0: 1.0\n",
            "precision for batch0: 1.0\n",
            "recall for batch0: 1.0\n",
            "f1 score for batch0: 1.0\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 4: 1.0\n",
            "precision for file 4: 1.0\n",
            "recall for file 4: 1.0\n",
            "f1 score for file 4: 1.0\n",
            "\n",
            "\n",
            "accuracy for batch 0: 0.875\n",
            "precision for batch0: 0.9523809523809523\n",
            "recall for batch0: 0.8695652173913043\n",
            "f1 score for batch0: 0.909090909090909\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 5: 0.9375\n",
            "precision for file 5: 0.9761904761904762\n",
            "recall for file 5: 0.9347826086956521\n",
            "f1 score for file 5: 0.9545454545454545\n",
            "\n",
            "\n",
            "accuracy for batch 0: 0.9375\n",
            "precision for batch0: 0.92\n",
            "recall for batch0: 1.0\n",
            "f1 score for batch0: 0.9583333333333334\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 6: 0.96875\n",
            "precision for file 6: 0.96\n",
            "recall for file 6: 1.0\n",
            "f1 score for file 6: 0.9791666666666667\n",
            "\n",
            "\n",
            "accuracy for batch 0: 1.0\n",
            "precision for batch0: 1.0\n",
            "recall for batch0: 1.0\n",
            "f1 score for batch0: 1.0\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 7: 1.0\n",
            "precision for file 7: 1.0\n",
            "recall for file 7: 1.0\n",
            "f1 score for file 7: 1.0\n",
            "\n",
            "\n",
            "accuracy for batch 0: 0.90625\n",
            "precision for batch0: 0.9545454545454546\n",
            "recall for batch0: 0.9130434782608695\n",
            "f1 score for batch0: 0.9333333333333332\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 8: 0.953125\n",
            "precision for file 8: 0.9772727272727273\n",
            "recall for file 8: 0.9565217391304348\n",
            "f1 score for file 8: 0.9666666666666666\n",
            "\n",
            "\n",
            "accuracy for batch 0: 0.9375\n",
            "precision for batch0: 0.9565217391304348\n",
            "recall for batch0: 0.9565217391304348\n",
            "f1 score for batch0: 0.9565217391304348\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 9: 0.96875\n",
            "precision for file 9: 0.9782608695652174\n",
            "recall for file 9: 0.9782608695652174\n",
            "f1 score for file 9: 0.9782608695652174\n",
            "\n",
            "\n",
            "accuracy for batch 0: 1.0\n",
            "precision for batch0: 1.0\n",
            "recall for batch0: 1.0\n",
            "f1 score for batch0: 1.0\n",
            "accuracy for batch 1: 1.0\n",
            "precision for batch1: 1.0\n",
            "recall for batch1: 1.0\n",
            "f1 score for batch1: 1.0\n",
            "\n",
            "accuracy for file 10: 1.0\n",
            "precision for file 10: 1.0\n",
            "recall for file 10: 1.0\n",
            "f1 score for file 10: 1.0\n",
            "\n",
            "\n",
            "test accuracy for all files: 96.96875%\n",
            "test precision for all files: 98.72493303797653%\n",
            "test recall for all files: 97.46837944664031%\n",
            "test f1 score for all files: 98.01652273343079%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9SS6sYV9eT8"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    }
  ]
}