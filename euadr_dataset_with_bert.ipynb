{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "euadr-dataset-with-bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrDStEeY7wVy/xEQEnomqX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hushee69/biobert-relation-extraction/blob/main/euadr_dataset_with_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asONPSIFkEx3",
        "outputId": "1c870d3d-e934-48f4-f6b8-cb946cc9876e"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoWs_ms5XRw-"
      },
      "source": [
        "import numpy as np;\r\n",
        "import pandas as pd;\r\n",
        "\r\n",
        "import torch;\r\n",
        "\r\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig;\r\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler;\r\n",
        "from transformers import get_linear_schedule_with_warmup;"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJEUllxpY7dM",
        "outputId": "236b4233-d9d7-4f39-a728-e5df532c0605"
      },
      "source": [
        "from google.colab import drive;\r\n",
        "\r\n",
        "drive.mount('/content/gdrive');"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcYLTaLhdGJ"
      },
      "source": [
        "path='/content/gdrive/MyDrive/biobert_re/';"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6iKt23StQFZ"
      },
      "source": [
        "device = torch.device('cuda');"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rH4qL4Cl_Mi"
      },
      "source": [
        "def accuracy(preds, labels):\r\n",
        "    preds = np.argmax(preds, axis=1).flatten();\r\n",
        "    labels = labels.flatten();\r\n",
        "\r\n",
        "    return np.sum(preds == labels) / len(labels);"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiR5EcY7YwkG"
      },
      "source": [
        "\"\"\"\r\n",
        "    Params:\r\n",
        "        filepath: path of the dataset\r\n",
        "        tokenizer: tokenizer to use\r\n",
        "        maxlen: maxlength of text\r\n",
        "\"\"\"\r\n",
        "def load_and_process_euadr_train_data(filepath, tokenizer, maxlen=512, train_percentage=0.7):\r\n",
        "    # load dataset\r\n",
        "    df = pd.read_csv(filepath, header=None, delimiter='\\t', names=['sentence', 'label']);\r\n",
        "\r\n",
        "    sentences = df.sentence.values;\r\n",
        "    labels = df.label.values;\r\n",
        "    \r\n",
        "    input_ids = [];\r\n",
        "    attention_masks = [];\r\n",
        "    \r\n",
        "    for sent in sentences:\r\n",
        "        encoded_dict = tokenizer.encode_plus(\r\n",
        "            sent,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=maxlen,\r\n",
        "            padding='max_length',\r\n",
        "            truncation=True,\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt'\r\n",
        "        );\r\n",
        "\r\n",
        "        input_ids.append(encoded_dict['input_ids']);\r\n",
        "        attention_masks.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "    # convert lists into tensors\r\n",
        "    input_ids = torch.cat(input_ids, dim=0);\r\n",
        "    attention_masks = torch.cat(attention_masks, dim=0);\r\n",
        "    labels_tensor = torch.tensor(labels);\r\n",
        "\r\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels_tensor);\r\n",
        "\r\n",
        "    train_size = int(train_percentage * len(dataset));\r\n",
        "    val_size = len(dataset) - train_size;\r\n",
        "\r\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size]);\r\n",
        "\r\n",
        "    return (train_dataset, val_dataset);"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5hxq5PBk3w_"
      },
      "source": [
        "def train_euadr_model(model, data, optimizer=AdamW, batch_size=32, epochs=3):\r\n",
        "    max_val_loss = np.float('inf');\r\n",
        "\r\n",
        "    train_ds = data[0];\r\n",
        "    val_ds = data[1];\r\n",
        "\r\n",
        "    train_dataloader = DataLoader(\r\n",
        "        train_ds,\r\n",
        "        sampler=RandomSampler(train_ds),\r\n",
        "        batch_size=batch_size\r\n",
        "    );\r\n",
        "\r\n",
        "    val_dataloader = DataLoader(\r\n",
        "        val_ds,\r\n",
        "        sampler=SequentialSampler(val_ds),\r\n",
        "        batch_size=batch_size\r\n",
        "    );\r\n",
        "\r\n",
        "    for e in range(epochs):\r\n",
        "        train_loss = 0;\r\n",
        "        train_acc = 0;\r\n",
        "\r\n",
        "        model.train();\r\n",
        "\r\n",
        "        optim = optimizer(model.parameters(), lr=2e-5, eps=1e-8);\r\n",
        "\r\n",
        "        scheduler = get_linear_schedule_with_warmup(\r\n",
        "            optim,\r\n",
        "            num_warmup_steps=0,\r\n",
        "            num_training_steps=len(train_dataloader) * epochs\r\n",
        "        );\r\n",
        "\r\n",
        "        for batch in train_dataloader:\r\n",
        "            b_input_ids = batch[0].to(device);\r\n",
        "            b_input_mask = batch[1].to(device);\r\n",
        "            b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "            model.zero_grad();\r\n",
        "\r\n",
        "            output = model(\r\n",
        "                b_input_ids,\r\n",
        "                token_type_ids=None,\r\n",
        "                attention_mask=b_input_mask,\r\n",
        "                labels=b_labels\r\n",
        "            );\r\n",
        "\r\n",
        "            loss = output['loss'];\r\n",
        "            preds = output['logits'].detach().cpu().numpy();\r\n",
        "            labels = b_labels.to('cpu').numpy();\r\n",
        "\r\n",
        "            train_loss += loss.item();\r\n",
        "            train_acc += accuracy(preds, labels);\r\n",
        "\r\n",
        "            loss.backward();\r\n",
        "\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0);\r\n",
        "\r\n",
        "            optim.step();\r\n",
        "\r\n",
        "            scheduler.step();\r\n",
        "        \r\n",
        "        avg_train_loss = train_loss / len(train_dataloader);\r\n",
        "        avg_train_acc = train_acc / len(train_dataloader);\r\n",
        "\r\n",
        "        print('average training loss for epoch: {}'.format(avg_train_loss));\r\n",
        "        print('average training accuracy for epoch: {}'.format(avg_train_acc));\r\n",
        "\r\n",
        "        # validation\r\n",
        "        val_loss = 0;\r\n",
        "        val_acc = 0;\r\n",
        "\r\n",
        "        model.eval();\r\n",
        "\r\n",
        "        for batch in val_dataloader:\r\n",
        "            b_input_ids = batch[0].to(device);\r\n",
        "            b_attention_mask = batch[1].to(device);\r\n",
        "            b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "            with torch.no_grad():\r\n",
        "                output = model(\r\n",
        "                    b_input_ids,\r\n",
        "                    token_type_ids=None,\r\n",
        "                    attention_mask=b_attention_mask,\r\n",
        "                    labels=b_labels\r\n",
        "                );\r\n",
        "            \r\n",
        "            loss = output['loss'];\r\n",
        "            preds = output['logits'].detach().cpu().numpy();\r\n",
        "            labels = b_labels.to('cpu').numpy();\r\n",
        "\r\n",
        "            val_loss += loss.item();\r\n",
        "            val_acc += accuracy(preds, labels);\r\n",
        "        \r\n",
        "        avg_val_loss = val_loss / len(val_dataloader);\r\n",
        "        avg_val_acc = val_acc / len(val_dataloader);\r\n",
        "\r\n",
        "        if avg_val_loss < max_val_loss:\r\n",
        "            max_val_loss = avg_val_loss;\r\n",
        "            torch.save(model.state_dict(), 'best_model.pt');\r\n",
        "\r\n",
        "        print('average validation loss for epoch: {}'.format(avg_val_loss));\r\n",
        "        print('average validation accuracy for epoch: {}'.format(avg_val_acc));"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYz30W6M16nb"
      },
      "source": [
        "**Training EU-ADR**\r\n",
        "\r\n",
        "Since there is not much data in EU-ADR dataset, we chose to have 3 epochs so as to prevent overfitting of data. This decision was not experimental since the authors advise fine-tuning using 2-4 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N2QWcTMkBT9",
        "outputId": "dc032194-b12f-4ac0-a58e-4df2c28e485b"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased');\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained(\r\n",
        "    'bert-base-uncased',\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions=False,\r\n",
        "    output_hidden_states=False\r\n",
        ");\r\n",
        "\r\n",
        "device = torch.device('cuda');\r\n",
        "model.cuda();\r\n",
        "\r\n",
        "epochs = 2;"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nPNQdAJi1Ff",
        "outputId": "47e79fcb-c9ae-45c4-f96b-236bab394176"
      },
      "source": [
        "for i in range(10):\r\n",
        "    data = load_and_process_euadr_train_data(path + 'euadr/' + str(i + 1) + '/train.tsv', tokenizer=tokenizer, maxlen=128);\r\n",
        "    train_euadr_model(model, data);"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training loss for epoch: 0.5833209412438529\n",
            "average training accuracy for epoch: 0.6729166666666667\n",
            "average validation loss for epoch: 0.5731187264124552\n",
            "average validation accuracy for epoch: 0.7291666666666666\n",
            "average training loss for epoch: 0.4882193420614515\n",
            "average training accuracy for epoch: 0.805952380952381\n",
            "average validation loss for epoch: 0.5841427644093832\n",
            "average validation accuracy for epoch: 0.7395833333333334\n",
            "average training loss for epoch: 0.42743650930268423\n",
            "average training accuracy for epoch: 0.8470238095238096\n",
            "average validation loss for epoch: 0.6461945374806722\n",
            "average validation accuracy for epoch: 0.7083333333333334\n",
            "average training loss for epoch: 0.43372587646756855\n",
            "average training accuracy for epoch: 0.8250000000000001\n",
            "average validation loss for epoch: 0.6031940579414368\n",
            "average validation accuracy for epoch: 0.75\n",
            "average training loss for epoch: 0.5161608925887516\n",
            "average training accuracy for epoch: 0.7845238095238096\n",
            "average validation loss for epoch: 0.4327192505200704\n",
            "average validation accuracy for epoch: 0.8333333333333334\n",
            "average training loss for epoch: 0.4849779009819031\n",
            "average training accuracy for epoch: 0.7967261904761905\n",
            "average validation loss for epoch: 0.47334055105845135\n",
            "average validation accuracy for epoch: 0.8125\n",
            "average training loss for epoch: 0.4077930450439453\n",
            "average training accuracy for epoch: 0.837202380952381\n",
            "average validation loss for epoch: 0.49485741058985394\n",
            "average validation accuracy for epoch: 0.7916666666666666\n",
            "average training loss for epoch: 0.36766694699014935\n",
            "average training accuracy for epoch: 0.8425595238095239\n",
            "average validation loss for epoch: 0.5188524425029755\n",
            "average validation accuracy for epoch: 0.7708333333333334\n",
            "average training loss for epoch: 0.4243500999041966\n",
            "average training accuracy for epoch: 0.8428859447004609\n",
            "average validation loss for epoch: 0.40654770533243817\n",
            "average validation accuracy for epoch: 0.8125\n",
            "average training loss for epoch: 0.37119715554373606\n",
            "average training accuracy for epoch: 0.8474942396313364\n",
            "average validation loss for epoch: 0.3851049641768138\n",
            "average validation accuracy for epoch: 0.8020833333333334\n",
            "average training loss for epoch: 0.3114877811500004\n",
            "average training accuracy for epoch: 0.8787442396313364\n",
            "average validation loss for epoch: 0.3732384244600932\n",
            "average validation accuracy for epoch: 0.84375\n",
            "average training loss for epoch: 0.3050573319196701\n",
            "average training accuracy for epoch: 0.869815668202765\n",
            "average validation loss for epoch: 0.41458359360694885\n",
            "average validation accuracy for epoch: 0.8020833333333334\n",
            "average training loss for epoch: 0.32396229675837923\n",
            "average training accuracy for epoch: 0.8705357142857143\n",
            "average validation loss for epoch: 0.38038110733032227\n",
            "average validation accuracy for epoch: 0.8333333333333334\n",
            "average training loss for epoch: 0.27762495407036375\n",
            "average training accuracy for epoch: 0.9017857142857143\n",
            "average validation loss for epoch: 0.40254613757133484\n",
            "average validation accuracy for epoch: 0.8020833333333334\n",
            "average training loss for epoch: 0.23349690437316895\n",
            "average training accuracy for epoch: 0.9330357142857143\n",
            "average validation loss for epoch: 0.4719330271085103\n",
            "average validation accuracy for epoch: 0.78125\n",
            "average training loss for epoch: 0.21249571229730332\n",
            "average training accuracy for epoch: 0.9285714285714286\n",
            "average validation loss for epoch: 0.44909263650576275\n",
            "average validation accuracy for epoch: 0.8020833333333334\n",
            "average training loss for epoch: 0.2796054759195873\n",
            "average training accuracy for epoch: 0.8883928571428571\n",
            "average validation loss for epoch: 0.2097317228714625\n",
            "average validation accuracy for epoch: 0.9166666666666666\n",
            "average training loss for epoch: 0.26800486019679476\n",
            "average training accuracy for epoch: 0.8973214285714286\n",
            "average validation loss for epoch: 0.24953696131706238\n",
            "average validation accuracy for epoch: 0.90625\n",
            "average training loss for epoch: 0.2590335692678179\n",
            "average training accuracy for epoch: 0.8973214285714286\n",
            "average validation loss for epoch: 0.26948800434668857\n",
            "average validation accuracy for epoch: 0.8854166666666666\n",
            "average training loss for epoch: 0.18004226045949118\n",
            "average training accuracy for epoch: 0.9419642857142857\n",
            "average validation loss for epoch: 0.2916051397720973\n",
            "average validation accuracy for epoch: 0.8854166666666666\n",
            "average training loss for epoch: 0.24010674016816275\n",
            "average training accuracy for epoch: 0.9196428571428571\n",
            "average validation loss for epoch: 0.19176344076792398\n",
            "average validation accuracy for epoch: 0.9375\n",
            "average training loss for epoch: 0.22952631967408316\n",
            "average training accuracy for epoch: 0.9151785714285714\n",
            "average validation loss for epoch: 0.20045744876066843\n",
            "average validation accuracy for epoch: 0.90625\n",
            "average training loss for epoch: 0.1899163829428809\n",
            "average training accuracy for epoch: 0.9330357142857143\n",
            "average validation loss for epoch: 0.24534165859222412\n",
            "average validation accuracy for epoch: 0.8958333333333334\n",
            "average training loss for epoch: 0.1949441603251866\n",
            "average training accuracy for epoch: 0.9285714285714286\n",
            "average validation loss for epoch: 0.2404868801434835\n",
            "average validation accuracy for epoch: 0.90625\n",
            "average training loss for epoch: 0.17899217243705476\n",
            "average training accuracy for epoch: 0.90625\n",
            "average validation loss for epoch: 0.17182011902332306\n",
            "average validation accuracy for epoch: 0.9479166666666666\n",
            "average training loss for epoch: 0.1558481284550258\n",
            "average training accuracy for epoch: 0.9375\n",
            "average validation loss for epoch: 0.13872218380371729\n",
            "average validation accuracy for epoch: 0.9583333333333334\n",
            "average training loss for epoch: 0.13378353895885603\n",
            "average training accuracy for epoch: 0.9464285714285714\n",
            "average validation loss for epoch: 0.25736699998378754\n",
            "average validation accuracy for epoch: 0.9270833333333334\n",
            "average training loss for epoch: 0.0768617210643632\n",
            "average training accuracy for epoch: 0.96875\n",
            "average validation loss for epoch: 0.5546037256717682\n",
            "average validation accuracy for epoch: 0.8645833333333334\n",
            "average training loss for epoch: 0.21197274965899332\n",
            "average training accuracy for epoch: 0.9196428571428571\n",
            "average validation loss for epoch: 0.09136115635434787\n",
            "average validation accuracy for epoch: 0.9583333333333334\n",
            "average training loss for epoch: 0.13822326170546667\n",
            "average training accuracy for epoch: 0.9419642857142857\n",
            "average validation loss for epoch: 0.1511648247639338\n",
            "average validation accuracy for epoch: 0.96875\n",
            "average training loss for epoch: 0.1279701255261898\n",
            "average training accuracy for epoch: 0.9419642857142857\n",
            "average validation loss for epoch: 0.10482590273022652\n",
            "average validation accuracy for epoch: 0.9583333333333334\n",
            "average training loss for epoch: 0.11718185804784298\n",
            "average training accuracy for epoch: 0.9508928571428571\n",
            "average validation loss for epoch: 0.17238258818785349\n",
            "average validation accuracy for epoch: 0.9479166666666666\n",
            "average training loss for epoch: 0.14074248181922094\n",
            "average training accuracy for epoch: 0.9553571428571429\n",
            "average validation loss for epoch: 0.23382074882586798\n",
            "average validation accuracy for epoch: 0.9270833333333334\n",
            "average training loss for epoch: 0.1332655383407005\n",
            "average training accuracy for epoch: 0.9508928571428571\n",
            "average validation loss for epoch: 0.3186827500661214\n",
            "average validation accuracy for epoch: 0.8854166666666666\n",
            "average training loss for epoch: 0.09133566224149295\n",
            "average training accuracy for epoch: 0.9776785714285714\n",
            "average validation loss for epoch: 0.28609132145841915\n",
            "average validation accuracy for epoch: 0.90625\n",
            "average training loss for epoch: 0.08715749079627651\n",
            "average training accuracy for epoch: 0.9776785714285714\n",
            "average validation loss for epoch: 0.23480659474929175\n",
            "average validation accuracy for epoch: 0.90625\n",
            "average training loss for epoch: 0.11027004250458308\n",
            "average training accuracy for epoch: 0.9508928571428571\n",
            "average validation loss for epoch: 0.22851932483414808\n",
            "average validation accuracy for epoch: 0.9479166666666666\n",
            "average training loss for epoch: 0.10983711733881917\n",
            "average training accuracy for epoch: 0.9642857142857143\n",
            "average validation loss for epoch: 0.22947276135285696\n",
            "average validation accuracy for epoch: 0.9479166666666666\n",
            "average training loss for epoch: 0.11586386018565722\n",
            "average training accuracy for epoch: 0.9598214285714286\n",
            "average validation loss for epoch: 0.2776478131612142\n",
            "average validation accuracy for epoch: 0.9375\n",
            "average training loss for epoch: 0.13476710606898581\n",
            "average training accuracy for epoch: 0.9419642857142857\n",
            "average validation loss for epoch: 0.22906052693724632\n",
            "average validation accuracy for epoch: 0.9479166666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WTXMD-l21ko"
      },
      "source": [
        "from IPython.display import display;"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl9LBgJ92Wlq"
      },
      "source": [
        "\"\"\"\r\n",
        "    Params:\r\n",
        "        filepath: path of the dataset\r\n",
        "        tokenizer: tokenizer to use\r\n",
        "        maxlen: maxlength of text\r\n",
        "\"\"\"\r\n",
        "def load_and_process_euadr_test_data(filepath, tokenizer, maxlen=512):\r\n",
        "    # load dataset\r\n",
        "    df = pd.read_csv(filepath, delimiter='\\t', index_col=0);\r\n",
        "\r\n",
        "    sentences = df.sentence.values;\r\n",
        "    labels = df.label.values;\r\n",
        "    \r\n",
        "    input_ids = [];\r\n",
        "    attention_masks = [];\r\n",
        "    \r\n",
        "    for sent in sentences:\r\n",
        "        encoded_dict = tokenizer.encode_plus(\r\n",
        "            sent,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=maxlen,\r\n",
        "            padding='max_length',\r\n",
        "            truncation=True,\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt'\r\n",
        "        );\r\n",
        "\r\n",
        "        input_ids.append(encoded_dict['input_ids']);\r\n",
        "        attention_masks.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "    # convert lists into tensors\r\n",
        "    input_ids = torch.cat(input_ids, dim=0);\r\n",
        "    attention_masks = torch.cat(attention_masks, dim=0);\r\n",
        "    labels_tensor = torch.tensor(labels);\r\n",
        "\r\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels_tensor);\r\n",
        "\r\n",
        "    return dataset;"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no5-wVSo5_ZV"
      },
      "source": [
        "def test_euadr_model(model, data, batch_size=32):\r\n",
        "    model.load_state_dict(torch.load('best_model.pt'));\r\n",
        "\r\n",
        "    model.eval();\r\n",
        "\r\n",
        "    ds = data;\r\n",
        "\r\n",
        "    test_dataloader = DataLoader(\r\n",
        "        ds,\r\n",
        "        sampler=SequentialSampler(ds),\r\n",
        "        batch_size=batch_size,\r\n",
        "    );\r\n",
        "\r\n",
        "    preds, real_labels = [], [];\r\n",
        "\r\n",
        "    total_accuracy_for_file = 0;\r\n",
        "    for batch_nbr, batch in enumerate(test_dataloader):\r\n",
        "        b_input_ids = batch[0].to(device);\r\n",
        "        b_input_mask = batch[1].to(device);\r\n",
        "        b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            outputs = model(\r\n",
        "                b_input_ids,\r\n",
        "                token_type_ids=None,\r\n",
        "                attention_mask=b_input_mask\r\n",
        "            );\r\n",
        "        \r\n",
        "        logits = outputs['logits'].detach().cpu().numpy();\r\n",
        "        label_ids = b_labels.to('cpu').numpy();\r\n",
        "        \r\n",
        "        batch_acc = accuracy(logits, label_ids);\r\n",
        "        total_accuracy_for_file += batch_acc;\r\n",
        "        print('accuracy for batch {}: {}'.format(batch_nbr, batch_acc));\r\n",
        "    \r\n",
        "    return total_accuracy_for_file / (batch_nbr + 1);"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKkzrVTw7dQK",
        "outputId": "208d00c1-b0fc-4be3-807a-bc0def4ac3f9"
      },
      "source": [
        "total_accuracy_for_files = 0;\r\n",
        "\r\n",
        "for i in range(10):\r\n",
        "    data = load_and_process_euadr_test_data(path + 'euadr/' + str(i + 1) + '/test.tsv', tokenizer=tokenizer, maxlen=128);\r\n",
        "    accuracy_per_file = test_euadr_model(model, data);\r\n",
        "    total_accuracy_for_files += accuracy_per_file;\r\n",
        "    print('accuracy for file {}: {}'.format((i + 1), accuracy_per_file));\r\n",
        "    print();\r\n",
        "\r\n",
        "# 10 files\r\n",
        "print('test accuracy for all files: {}%'.format(((total_accuracy_for_files) / 10) * 100));"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy for batch 0: 1.0\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 1: 1.0\n",
            "\n",
            "accuracy for batch 0: 0.96875\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 2: 0.984375\n",
            "\n",
            "accuracy for batch 0: 1.0\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 3: 1.0\n",
            "\n",
            "accuracy for batch 0: 0.96875\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 4: 0.984375\n",
            "\n",
            "accuracy for batch 0: 0.90625\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 5: 0.953125\n",
            "\n",
            "accuracy for batch 0: 0.9375\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 6: 0.96875\n",
            "\n",
            "accuracy for batch 0: 0.96875\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 7: 0.984375\n",
            "\n",
            "accuracy for batch 0: 0.96875\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 8: 0.984375\n",
            "\n",
            "accuracy for batch 0: 0.96875\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 9: 0.984375\n",
            "\n",
            "accuracy for batch 0: 1.0\n",
            "accuracy for batch 1: 1.0\n",
            "accuracy for file 10: 1.0\n",
            "\n",
            "test accuracy for all files: 98.4375%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9SS6sYV9eT8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}