{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gad-dataset-with-bert.ipynb",
      "provenance": [],
      "mount_file_id": "1XNyKyu3e8WrOW5w0rNqXxZOoKV3LBoZ5",
      "authorship_tag": "ABX9TyOxgpP7b55/MDkY4N2wyaPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hushee69/biobert-relation-extraction/blob/main/gad_dataset_with_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNU6EMsKnqng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asONPSIFkEx3",
        "outputId": "492b4a41-6c05-4128-b2ae-eeb8e0d71796"
      },
      "source": [
        "# datasets link: https://drive.google.com/open?id=1-jDKGcXREb2X9xTFnuiJ36PvsqoyHWcw\r\n",
        "\r\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VzzbgtPnv1Y"
      },
      "source": [
        "import numpy as np;\r\n",
        "import pandas as pd;\r\n",
        "\r\n",
        "import torch;\r\n",
        "\r\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig;\r\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler;\r\n",
        "from transformers import get_linear_schedule_with_warmup;\r\n",
        "\r\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score;"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWQSAHb2nwOg",
        "outputId": "3a9d328c-03be-4d6c-e5b5-a4896c212f11"
      },
      "source": [
        "from google.colab import drive;\r\n",
        "\r\n",
        "drive.mount('/content/gdrive');"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSROJxLHn2Tj"
      },
      "source": [
        "path='/content/gdrive/MyDrive/biobert_re/';"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhcSAgfon-cC"
      },
      "source": [
        "device = torch.device('cuda');\r\n",
        "\r\n",
        "SEED = 42;\r\n",
        "\r\n",
        "torch.manual_seed(SEED);\r\n",
        "torch.backends.cudnn.deterministic = True;"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DVOubFsoADX"
      },
      "source": [
        "\"\"\"\r\n",
        "    Params:\r\n",
        "        filepath: path of the dataset\r\n",
        "        tokenizer: tokenizer to use\r\n",
        "        maxlen: maxlength of text\r\n",
        "\"\"\"\r\n",
        "def load_and_process_gad_train_data(filepath, tokenizer, maxlen=512, train_percentage=0.7):\r\n",
        "    # load dataset\r\n",
        "    df = pd.read_csv(filepath, header=None, delimiter='\\t', names=['sentence', 'label']);\r\n",
        "\r\n",
        "    sentences = df.sentence.values;\r\n",
        "    labels = df.label.values;\r\n",
        "    \r\n",
        "    input_ids = [];\r\n",
        "    attention_masks = [];\r\n",
        "    \r\n",
        "    for sent in sentences:\r\n",
        "        encoded_dict = tokenizer.encode_plus(\r\n",
        "            sent,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=maxlen,\r\n",
        "            padding='max_length',\r\n",
        "            truncation=True,\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt'\r\n",
        "        );\r\n",
        "\r\n",
        "        input_ids.append(encoded_dict['input_ids']);\r\n",
        "        attention_masks.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "    # convert lists into tensors\r\n",
        "    input_ids = torch.cat(input_ids, dim=0);\r\n",
        "    attention_masks = torch.cat(attention_masks, dim=0);\r\n",
        "    labels_tensor = torch.tensor(labels);\r\n",
        "\r\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels_tensor);\r\n",
        "\r\n",
        "    train_size = int(train_percentage * len(dataset));\r\n",
        "    val_size = len(dataset) - train_size;\r\n",
        "\r\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size]);\r\n",
        "\r\n",
        "    return (train_dataset, val_dataset);"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPzh6Gq_pW12"
      },
      "source": [
        "def train_gad_model(model, data, optimizer=AdamW, batch_size=32, epochs=3):\r\n",
        "    max_val_loss = np.float('inf');\r\n",
        "\r\n",
        "    train_ds = data[0];\r\n",
        "    val_ds = data[1];\r\n",
        "\r\n",
        "    train_dataloader = DataLoader(\r\n",
        "        train_ds,\r\n",
        "        sampler=RandomSampler(train_ds),\r\n",
        "        batch_size=batch_size\r\n",
        "    );\r\n",
        "\r\n",
        "    val_dataloader = DataLoader(\r\n",
        "        val_ds,\r\n",
        "        sampler=SequentialSampler(val_ds),\r\n",
        "        batch_size=batch_size\r\n",
        "    );\r\n",
        "\r\n",
        "    for e in range(epochs):\r\n",
        "        train_loss = 0;\r\n",
        "        train_acc = 0;\r\n",
        "\r\n",
        "        model.train();\r\n",
        "\r\n",
        "        optim = optimizer(model.parameters(), lr=2e-5, eps=1e-8);\r\n",
        "\r\n",
        "        scheduler = get_linear_schedule_with_warmup(\r\n",
        "            optim,\r\n",
        "            num_warmup_steps=0,\r\n",
        "            num_training_steps=len(train_dataloader) * epochs\r\n",
        "        );\r\n",
        "\r\n",
        "        for batch in train_dataloader:\r\n",
        "            b_input_ids = batch[0].to(device);\r\n",
        "            b_input_mask = batch[1].to(device);\r\n",
        "            b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "            model.zero_grad();\r\n",
        "\r\n",
        "            output = model(\r\n",
        "                b_input_ids,\r\n",
        "                token_type_ids=None,\r\n",
        "                attention_mask=b_input_mask,\r\n",
        "                labels=b_labels\r\n",
        "            );\r\n",
        "\r\n",
        "            loss = output['loss'];\r\n",
        "            preds = output['logits'].detach().cpu().numpy();\r\n",
        "            labels = b_labels.to('cpu').numpy();\r\n",
        "            preds = np.argmax(preds, axis=1);\r\n",
        "\r\n",
        "            train_loss += loss.item();\r\n",
        "            train_acc += accuracy_score(labels, preds);\r\n",
        "\r\n",
        "            loss.backward();\r\n",
        "\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0);\r\n",
        "\r\n",
        "            optim.step();\r\n",
        "\r\n",
        "            scheduler.step();\r\n",
        "        \r\n",
        "        avg_train_loss = train_loss / len(train_dataloader);\r\n",
        "        avg_train_acc = train_acc / len(train_dataloader);\r\n",
        "\r\n",
        "        print('average training loss for epoch: {}'.format(avg_train_loss));\r\n",
        "        print('average training accuracy for epoch: {}'.format(avg_train_acc));\r\n",
        "\r\n",
        "        # validation\r\n",
        "        val_loss = 0;\r\n",
        "        val_acc = 0;\r\n",
        "\r\n",
        "        model.eval();\r\n",
        "\r\n",
        "        for batch in val_dataloader:\r\n",
        "            b_input_ids = batch[0].to(device);\r\n",
        "            b_attention_mask = batch[1].to(device);\r\n",
        "            b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "            with torch.no_grad():\r\n",
        "                output = model(\r\n",
        "                    b_input_ids,\r\n",
        "                    token_type_ids=None,\r\n",
        "                    attention_mask=b_attention_mask,\r\n",
        "                    labels=b_labels\r\n",
        "                );\r\n",
        "            \r\n",
        "            loss = output['loss'];\r\n",
        "            preds = output['logits'].detach().cpu().numpy();\r\n",
        "            labels = b_labels.to('cpu').numpy();\r\n",
        "            preds = np.argmax(preds, axis=1);\r\n",
        "\r\n",
        "            val_loss += loss.item();\r\n",
        "            val_acc += accuracy_score(labels, preds);\r\n",
        "        \r\n",
        "        avg_val_loss = val_loss / len(val_dataloader);\r\n",
        "        avg_val_acc = val_acc / len(val_dataloader);\r\n",
        "\r\n",
        "        if avg_val_loss < max_val_loss:\r\n",
        "            max_val_loss = avg_val_loss;\r\n",
        "            torch.save(model.state_dict(), 'best_model.pt');\r\n",
        "\r\n",
        "        print('average validation loss for epoch: {}'.format(avg_val_loss));\r\n",
        "        print('average validation accuracy for epoch: {}'.format(avg_val_acc));"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Icy_Qfjoqs5",
        "outputId": "697183cf-3e1e-4386-fde3-2233cb0b05a1"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased');\r\n",
        "\r\n",
        "model = BertForSequenceClassification.from_pretrained(\r\n",
        "    'bert-base-uncased',\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions=False,\r\n",
        "    output_hidden_states=False\r\n",
        ");\r\n",
        "\r\n",
        "model.cuda();\r\n",
        "\r\n",
        "epochs = 2;"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc6BqA3Bocxg",
        "outputId": "25d69724-bca5-46a4-a868-b4166437873f"
      },
      "source": [
        "for i in range(10):\r\n",
        "    data = load_and_process_gad_train_data(path + 'GAD/' + str(i + 1) + '/train.tsv', tokenizer=tokenizer, maxlen=128);\r\n",
        "    train_gad_model(model, data);"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training loss for epoch: 0.6302057779970623\n",
            "average training accuracy for epoch: 0.6412356321839081\n",
            "average validation loss for epoch: 0.6741531868775685\n",
            "average validation accuracy for epoch: 0.6504928315412186\n",
            "average training loss for epoch: 0.5262822040489742\n",
            "average training accuracy for epoch: 0.751539408866995\n",
            "average validation loss for epoch: 0.5387268020047082\n",
            "average validation accuracy for epoch: 0.7413754480286738\n",
            "average training loss for epoch: 0.4455633171967098\n",
            "average training accuracy for epoch: 0.8008620689655173\n",
            "average validation loss for epoch: 0.5501131024625566\n",
            "average validation accuracy for epoch: 0.7470206093189964\n",
            "average training loss for epoch: 0.44360336647147225\n",
            "average training accuracy for epoch: 0.803745894909688\n",
            "average validation loss for epoch: 0.3960328121980031\n",
            "average validation accuracy for epoch: 0.8263888888888888\n",
            "average training loss for epoch: 0.3742846606742768\n",
            "average training accuracy for epoch: 0.8443144499178983\n",
            "average validation loss for epoch: 0.425465319554011\n",
            "average validation accuracy for epoch: 0.8215277777777777\n",
            "average training loss for epoch: 0.3106601119041443\n",
            "average training accuracy for epoch: 0.8743431855500821\n",
            "average validation loss for epoch: 0.4230558759636349\n",
            "average validation accuracy for epoch: 0.8333333333333334\n",
            "average training loss for epoch: 0.338654136515799\n",
            "average training accuracy for epoch: 0.8620792282430213\n",
            "average validation loss for epoch: 0.3329163289732403\n",
            "average validation accuracy for epoch: 0.8625\n",
            "average training loss for epoch: 0.27377918994142897\n",
            "average training accuracy for epoch: 0.8960385878489328\n",
            "average validation loss for epoch: 0.37762618876165815\n",
            "average validation accuracy for epoch: 0.8541666666666666\n",
            "average training loss for epoch: 0.21682997839081855\n",
            "average training accuracy for epoch: 0.9204741379310345\n",
            "average validation loss for epoch: 0.31573031379116906\n",
            "average validation accuracy for epoch: 0.8854166666666666\n",
            "average training loss for epoch: 0.2579237041374048\n",
            "average training accuracy for epoch: 0.9031814449917899\n",
            "average validation loss for epoch: 0.20062578986916277\n",
            "average validation accuracy for epoch: 0.9284722222222223\n",
            "average training loss for epoch: 0.1938176151897226\n",
            "average training accuracy for epoch: 0.9270833333333334\n",
            "average validation loss for epoch: 0.26648469128542474\n",
            "average validation accuracy for epoch: 0.9125\n",
            "average training loss for epoch: 0.15526511743664742\n",
            "average training accuracy for epoch: 0.9448788998357964\n",
            "average validation loss for epoch: 0.2590112552046776\n",
            "average validation accuracy for epoch: 0.9166666666666666\n",
            "average training loss for epoch: 0.1689628311565944\n",
            "average training accuracy for epoch: 0.9392241379310344\n",
            "average validation loss for epoch: 0.16348981271601384\n",
            "average validation accuracy for epoch: 0.9506944444444444\n",
            "average training loss for epoch: 0.1421848441163699\n",
            "average training accuracy for epoch: 0.9487171592775041\n",
            "average validation loss for epoch: 0.2798958696631922\n",
            "average validation accuracy for epoch: 0.9097222222222222\n",
            "average training loss for epoch: 0.10197699464901927\n",
            "average training accuracy for epoch: 0.9651785714285714\n",
            "average validation loss for epoch: 0.3075910245430552\n",
            "average validation accuracy for epoch: 0.9194444444444444\n",
            "average training loss for epoch: 0.15920773707330227\n",
            "average training accuracy for epoch: 0.9496100164203612\n",
            "average validation loss for epoch: 0.11667761664009757\n",
            "average validation accuracy for epoch: 0.9597222222222223\n",
            "average training loss for epoch: 0.12555478128737638\n",
            "average training accuracy for epoch: 0.961545566502463\n",
            "average validation loss for epoch: 0.12041870861624678\n",
            "average validation accuracy for epoch: 0.9666666666666667\n",
            "average training loss for epoch: 0.09253959313611544\n",
            "average training accuracy for epoch: 0.9711001642036126\n",
            "average validation loss for epoch: 0.19283915646891628\n",
            "average validation accuracy for epoch: 0.9541666666666667\n",
            "average training loss for epoch: 0.12932020352177676\n",
            "average training accuracy for epoch: 0.9616071428571429\n",
            "average validation loss for epoch: 0.11809807305689901\n",
            "average validation accuracy for epoch: 0.96875\n",
            "average training loss for epoch: 0.08663742281940012\n",
            "average training accuracy for epoch: 0.9731834975369459\n",
            "average validation loss for epoch: 0.1292198097359182\n",
            "average validation accuracy for epoch: 0.9659722222222222\n",
            "average training loss for epoch: 0.07194409898615309\n",
            "average training accuracy for epoch: 0.9770217569786536\n",
            "average validation loss for epoch: 0.13612198564741346\n",
            "average validation accuracy for epoch: 0.9673611111111111\n",
            "average training loss for epoch: 0.11360398560008478\n",
            "average training accuracy for epoch: 0.9702380952380952\n",
            "average validation loss for epoch: 0.07578518255096343\n",
            "average validation accuracy for epoch: 0.9791666666666666\n",
            "average training loss for epoch: 0.07491710006946786\n",
            "average training accuracy for epoch: 0.9809215927750411\n",
            "average validation loss for epoch: 0.08669209816855275\n",
            "average validation accuracy for epoch: 0.9756944444444444\n",
            "average training loss for epoch: 0.07065160428395584\n",
            "average training accuracy for epoch: 0.981547619047619\n",
            "average validation loss for epoch: 0.1632796608661819\n",
            "average validation accuracy for epoch: 0.9652777777777778\n",
            "average training loss for epoch: 0.09485404060798741\n",
            "average training accuracy for epoch: 0.9746715927750411\n",
            "average validation loss for epoch: 0.07233385877269838\n",
            "average validation accuracy for epoch: 0.98125\n",
            "average training loss for epoch: 0.07163797760537514\n",
            "average training accuracy for epoch: 0.9803571428571428\n",
            "average validation loss for epoch: 0.07173723979697873\n",
            "average validation accuracy for epoch: 0.9826388888888888\n",
            "average training loss for epoch: 0.05299087056579689\n",
            "average training accuracy for epoch: 0.985950328407225\n",
            "average validation loss for epoch: 0.1326646018294721\n",
            "average validation accuracy for epoch: 0.9701388888888889\n",
            "average training loss for epoch: 0.07972850539122842\n",
            "average training accuracy for epoch: 0.9791666666666666\n",
            "average validation loss for epoch: 0.043442271776600844\n",
            "average validation accuracy for epoch: 0.9868055555555556\n",
            "average training loss for epoch: 0.05258660755907407\n",
            "average training accuracy for epoch: 0.9871825396825397\n",
            "average validation loss for epoch: 0.047973581038079124\n",
            "average validation accuracy for epoch: 0.9881944444444445\n",
            "average training loss for epoch: 0.046397133145364935\n",
            "average training accuracy for epoch: 0.9880753968253968\n",
            "average validation loss for epoch: 0.10209741123091792\n",
            "average validation accuracy for epoch: 0.9798611111111111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxKhAhlMohGw"
      },
      "source": [
        "\"\"\"\r\n",
        "    Params:\r\n",
        "        filepath: path of the dataset\r\n",
        "        tokenizer: tokenizer to use\r\n",
        "        maxlen: maxlength of text\r\n",
        "\"\"\"\r\n",
        "def load_and_process_gad_test_data(filepath, tokenizer, maxlen=512):\r\n",
        "    # load dataset\r\n",
        "    df = pd.read_csv(filepath, delimiter='\\t', header=None, names=['wrong_index', 'sentence', 'label']);\r\n",
        "\r\n",
        "    sentences = df.sentence.values;\r\n",
        "    labels = df.label.values;\r\n",
        "    \r\n",
        "    input_ids = [];\r\n",
        "    attention_masks = [];\r\n",
        "    \r\n",
        "    for sent in sentences:\r\n",
        "        encoded_dict = tokenizer.encode_plus(\r\n",
        "            sent,\r\n",
        "            add_special_tokens=True,\r\n",
        "            max_length=maxlen,\r\n",
        "            padding='max_length',\r\n",
        "            truncation=True,\r\n",
        "            return_attention_mask=True,\r\n",
        "            return_tensors='pt'\r\n",
        "        );\r\n",
        "\r\n",
        "        input_ids.append(encoded_dict['input_ids']);\r\n",
        "        attention_masks.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "    # convert lists into tensors\r\n",
        "    input_ids = torch.cat(input_ids, dim=0);\r\n",
        "    attention_masks = torch.cat(attention_masks, dim=0);\r\n",
        "    labels_tensor = torch.tensor(labels);\r\n",
        "\r\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels_tensor);\r\n",
        "\r\n",
        "    return dataset;"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFgqv59sR7e"
      },
      "source": [
        "def test_euadr_model(model, data, batch_size=32):\r\n",
        "    model.load_state_dict(torch.load('best_model.pt'));\r\n",
        "\r\n",
        "    model.eval();\r\n",
        "\r\n",
        "    ds = data;\r\n",
        "\r\n",
        "    test_dataloader = DataLoader(\r\n",
        "        ds,\r\n",
        "        sampler=SequentialSampler(ds),\r\n",
        "        batch_size=batch_size,\r\n",
        "    );\r\n",
        "\r\n",
        "    preds_list, real_labels_list = [], [];\r\n",
        "\r\n",
        "    for batch_nbr, batch in enumerate(test_dataloader):\r\n",
        "        b_input_ids = batch[0].to(device);\r\n",
        "        b_input_mask = batch[1].to(device);\r\n",
        "        b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            outputs = model(\r\n",
        "                b_input_ids,\r\n",
        "                token_type_ids=None,\r\n",
        "                attention_mask=b_input_mask\r\n",
        "            );\r\n",
        "        \r\n",
        "        logits = outputs['logits'].detach().cpu().numpy();\r\n",
        "        label_ids = b_labels.to('cpu').numpy();\r\n",
        "        preds = np.argmax(logits, axis=1);\r\n",
        "\r\n",
        "        preds_list.append(preds);\r\n",
        "        real_labels_list.append(label_ids);\r\n",
        "\r\n",
        "        print('finished batch {}'.format(batch_nbr));\r\n",
        "    \r\n",
        "    ret = (\r\n",
        "        preds_list,\r\n",
        "        real_labels_list\r\n",
        "    );\r\n",
        "    \r\n",
        "    return ret;"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Qb4xU5wsUf1",
        "outputId": "290f388d-803e-4491-9455-7b2b569fb775"
      },
      "source": [
        "# load test_accumulated.tsv file\r\n",
        "avg_test_accuracy = 0;\r\n",
        "\r\n",
        "test_data = load_and_process_gad_test_data(path + 'GAD/test_accumulated.tsv', tokenizer=tokenizer);\r\n",
        "\r\n",
        "ret = test_euadr_model(model, test_data);"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "finished batch 0\n",
            "finished batch 1\n",
            "finished batch 2\n",
            "finished batch 3\n",
            "finished batch 4\n",
            "finished batch 5\n",
            "finished batch 6\n",
            "finished batch 7\n",
            "finished batch 8\n",
            "finished batch 9\n",
            "finished batch 10\n",
            "finished batch 11\n",
            "finished batch 12\n",
            "finished batch 13\n",
            "finished batch 14\n",
            "finished batch 15\n",
            "finished batch 16\n",
            "finished batch 17\n",
            "finished batch 18\n",
            "finished batch 19\n",
            "finished batch 20\n",
            "finished batch 21\n",
            "finished batch 22\n",
            "finished batch 23\n",
            "finished batch 24\n",
            "finished batch 25\n",
            "finished batch 26\n",
            "finished batch 27\n",
            "finished batch 28\n",
            "finished batch 29\n",
            "finished batch 30\n",
            "finished batch 31\n",
            "finished batch 32\n",
            "finished batch 33\n",
            "finished batch 34\n",
            "finished batch 35\n",
            "finished batch 36\n",
            "finished batch 37\n",
            "finished batch 38\n",
            "finished batch 39\n",
            "finished batch 40\n",
            "finished batch 41\n",
            "finished batch 42\n",
            "finished batch 43\n",
            "finished batch 44\n",
            "finished batch 45\n",
            "finished batch 46\n",
            "finished batch 47\n",
            "finished batch 48\n",
            "finished batch 49\n",
            "finished batch 50\n",
            "finished batch 51\n",
            "finished batch 52\n",
            "finished batch 53\n",
            "finished batch 54\n",
            "finished batch 55\n",
            "finished batch 56\n",
            "finished batch 57\n",
            "finished batch 58\n",
            "finished batch 59\n",
            "finished batch 60\n",
            "finished batch 61\n",
            "finished batch 62\n",
            "finished batch 63\n",
            "finished batch 64\n",
            "finished batch 65\n",
            "finished batch 66\n",
            "finished batch 67\n",
            "finished batch 68\n",
            "finished batch 69\n",
            "finished batch 70\n",
            "finished batch 71\n",
            "finished batch 72\n",
            "finished batch 73\n",
            "finished batch 74\n",
            "finished batch 75\n",
            "finished batch 76\n",
            "finished batch 77\n",
            "finished batch 78\n",
            "finished batch 79\n",
            "finished batch 80\n",
            "finished batch 81\n",
            "finished batch 82\n",
            "finished batch 83\n",
            "finished batch 84\n",
            "finished batch 85\n",
            "finished batch 86\n",
            "finished batch 87\n",
            "finished batch 88\n",
            "finished batch 89\n",
            "finished batch 90\n",
            "finished batch 91\n",
            "finished batch 92\n",
            "finished batch 93\n",
            "finished batch 94\n",
            "finished batch 95\n",
            "finished batch 96\n",
            "finished batch 97\n",
            "finished batch 98\n",
            "finished batch 99\n",
            "finished batch 100\n",
            "finished batch 101\n",
            "finished batch 102\n",
            "finished batch 103\n",
            "finished batch 104\n",
            "finished batch 105\n",
            "finished batch 106\n",
            "finished batch 107\n",
            "finished batch 108\n",
            "finished batch 109\n",
            "finished batch 110\n",
            "finished batch 111\n",
            "finished batch 112\n",
            "finished batch 113\n",
            "finished batch 114\n",
            "finished batch 115\n",
            "finished batch 116\n",
            "finished batch 117\n",
            "finished batch 118\n",
            "finished batch 119\n",
            "finished batch 120\n",
            "finished batch 121\n",
            "finished batch 122\n",
            "finished batch 123\n",
            "finished batch 124\n",
            "finished batch 125\n",
            "finished batch 126\n",
            "finished batch 127\n",
            "finished batch 128\n",
            "finished batch 129\n",
            "finished batch 130\n",
            "finished batch 131\n",
            "finished batch 132\n",
            "finished batch 133\n",
            "finished batch 134\n",
            "finished batch 135\n",
            "finished batch 136\n",
            "finished batch 137\n",
            "finished batch 138\n",
            "finished batch 139\n",
            "finished batch 140\n",
            "finished batch 141\n",
            "finished batch 142\n",
            "finished batch 143\n",
            "finished batch 144\n",
            "finished batch 145\n",
            "finished batch 146\n",
            "finished batch 147\n",
            "finished batch 148\n",
            "finished batch 149\n",
            "finished batch 150\n",
            "finished batch 151\n",
            "finished batch 152\n",
            "finished batch 153\n",
            "finished batch 154\n",
            "finished batch 155\n",
            "finished batch 156\n",
            "finished batch 157\n",
            "finished batch 158\n",
            "finished batch 159\n",
            "finished batch 160\n",
            "finished batch 161\n",
            "finished batch 162\n",
            "finished batch 163\n",
            "finished batch 164\n",
            "finished batch 165\n",
            "finished batch 166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czRj4lzVsUis",
        "outputId": "6d3e5ea2-4cad-43dc-9269-a6763e3c10d9"
      },
      "source": [
        "preds = np.concatenate(ret[0]);\r\n",
        "real = np.concatenate(ret[1]);\r\n",
        "\r\n",
        "print('test predictions: {}'.format(preds));\r\n",
        "print('real values: {}'.format(real));\r\n",
        "\r\n",
        "test_acc = accuracy_score(real, preds);\r\n",
        "test_prec = precision_score(real, preds);\r\n",
        "test_rec = recall_score(real, preds);\r\n",
        "test_f1 = f1_score(real, preds);\r\n",
        "\r\n",
        "print('test accuracy: {}'.format(test_acc));\r\n",
        "print('test precision: {}'.format(test_prec));\r\n",
        "print('test recall: {}'.format(test_rec));\r\n",
        "print('test f1: {}'.format(test_f1));"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test predictions: [1 1 1 ... 0 0 0]\n",
            "real values: [1 1 1 ... 0 0 0]\n",
            "test accuracy: 0.9928705440900563\n",
            "test precision: 0.9971212666426772\n",
            "test recall: 0.9892895394501964\n",
            "test f1: 0.9931899641577061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Ohky-l03CP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}