{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "relation_extraction_with_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMIMvjgbvL/KjHXigBb1icR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hushee69/biobert-relation-extraction/blob/main/relation_extraction_with_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq77nK9jcQ9N",
        "outputId": "3c710d01-3317-4304-9780-748d8e3d4029"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhDoyq5vZfxo"
      },
      "source": [
        "import numpy as np;\r\n",
        "import pandas as pd;\r\n",
        "\r\n",
        "import torch;\r\n",
        "\r\n",
        "from transformers import BertTokenizer;"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D70tDuBgcNLW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "13f487af-2e02-498f-d9af-e9a8accb775b"
      },
      "source": [
        "# load dataset\r\n",
        "# labels = {0, 1}\r\n",
        "# relation between gene and disease if label = 0\r\n",
        "# and vice versa is true\r\n",
        "df = pd.read_csv('./train.tsv', delimiter='\\t', header=None, names=['text', 'label']);\r\n",
        "df.sample(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2774</th>\n",
              "      <td>in Japan the [-215G &gt; A; IVS3 + 2T &gt; C] mutati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>Our data suggest that the @GENE$ Y402H polymor...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2179</th>\n",
              "      <td>The above described findings indicate that Arg...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3800</th>\n",
              "      <td>In summary, a rare P387L variant of the PTP-1B...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>The @GENE$ Gly146Ala variation may constitute ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text  label\n",
              "2774  in Japan the [-215G > A; IVS3 + 2T > C] mutati...      1\n",
              "494   Our data suggest that the @GENE$ Y402H polymor...      1\n",
              "2179  The above described findings indicate that Arg...      0\n",
              "3800  In summary, a rare P387L variant of the PTP-1B...      1\n",
              "1450  The @GENE$ Gly146Ala variation may constitute ...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB3LJXmjdmwI",
        "outputId": "f2e43a62-65a7-4b9d-e728-54e595b5afaa"
      },
      "source": [
        "print(df.loc[df.label == 1].sample(5));"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   text  label\n",
            "3369  Although SERT genotype was not reliably associ...      1\n",
            "419          @GENE$ may be an important @DISEASE$ gene.      1\n",
            "522   our data suggests that the @GENE$ D allele is ...      1\n",
            "1641  We therefore conclude that Tnfsf4 underlies At...      1\n",
            "1665  No differences in frequencies of the @GENE$ de...      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CBRZe_8d5sc",
        "outputId": "1060b98e-4544-41d4-9c2e-adb6f9579a9a"
      },
      "source": [
        "print(df.loc[df.label == 0].sample(5));"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   text  label\n",
            "493   IVS14A and 451Q @DISEASE$ of @GENE$ gene were ...      0\n",
            "3250  Considering population stratification, previou...      0\n",
            "4599  In conclusion, these findings suggest that @GE...      0\n",
            "3700  Although the studied polymorphisms have been p...      0\n",
            "2411  the ACE2 T allele confers a high risk for @DIS...      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bsgcqKzfBxA"
      },
      "source": [
        "sentences = df.text.values;\r\n",
        "sent_labels = df.label.values;"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOUkSZtwfK_G"
      },
      "source": [
        "# tokenize\r\n",
        "# add special tokens [CLS] to start of sentence and [SEP] to the end of each sentence\r\n",
        "# pad and truncate to single constant length\r\n",
        "# explicitly differentiate real tokens from padding tokens with 'attention mask'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soa1OFeHfNxg",
        "outputId": "c1016a40-d1a9-4da4-ed79-bacec70d3089"
      },
      "source": [
        "# find maximum length\r\n",
        "# BERT accepts 512 as maximum sentence length\r\n",
        "max_len = 0;\r\n",
        "\r\n",
        "for i, sent in enumerate(sentences):\r\n",
        "    if len(sent) > max_len:\r\n",
        "        max_len = len(sent);\r\n",
        "        idx = i;\r\n",
        "\r\n",
        "\r\n",
        "print('maximum sentence length: {}'.format(max_len));\r\n",
        "print('index: {}'.format(idx));"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maximum sentence length: 538\n",
            "index: 3171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVrVTxnggWWO",
        "outputId": "e918027b-bf86-43fe-824b-c48b367c79b9"
      },
      "source": [
        "print(sentences[182]);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The AA genotype of HMOX1 reduced the incidence of @DISEASE$, possibly due to the high expression level of @GENE$.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSAll8xFgoJV"
      },
      "source": [
        "# we will truncate to 128\r\n",
        "if max_len > 512:\r\n",
        "    max_len = 128;"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KwNFk7RhCWQ"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased');"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sBPEVHPgwmC",
        "outputId": "a43f1eba-9531-4e8c-c336-880d07e0bd34"
      },
      "source": [
        "input_ids = [];\r\n",
        "attention_masks = [];\r\n",
        "\r\n",
        "for sent in sentences:\r\n",
        "    encoded_dict = tokenizer.encode_plus(\r\n",
        "        sent,\r\n",
        "        add_special_tokens=True,\r\n",
        "        max_length=max_len,\r\n",
        "        padding='max_length',\r\n",
        "        truncation=True,\r\n",
        "        return_attention_mask=True,\r\n",
        "        return_tensors='pt'\r\n",
        "    );\r\n",
        "\r\n",
        "    input_ids.append(encoded_dict['input_ids']);\r\n",
        "    attention_masks.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "# convert lists into tensors\r\n",
        "input_ids = torch.cat(input_ids, dim=0);\r\n",
        "attention_masks = torch.cat(attention_masks, dim=0);\r\n",
        "labels = torch.tensor(sent_labels);\r\n",
        "\r\n",
        "print('original sentence: {}'.format(sentences[0]));\r\n",
        "print('tokenized: {}'.format(input_ids[0].shape));"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence: this study proposes that A/A genotype at position -607 in @GENE$ gene can be used as a new genetic maker in Thai population for predicting @DISEASE$ development.\n",
            "tokenized: torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2uOcuUji0SW"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split;"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j2U_AP4i5Dy",
        "outputId": "39f27676-e59b-42f9-c330-69e3b00321d7"
      },
      "source": [
        "dataset = TensorDataset(input_ids, attention_masks, labels);\r\n",
        "\r\n",
        "train_size = int(0.9 * len(dataset));\r\n",
        "val_size = len(dataset) - train_size;\r\n",
        "\r\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size]);\r\n",
        "print('{:>5,} training samples'.format(train_size));\r\n",
        "print('{:>5,} validation samples'.format(val_size));"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4,316 training samples\n",
            "  480 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYv6_0BIlO87"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler;"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S42VYJAil4MW"
      },
      "source": [
        "batch_size = 32;\r\n",
        "\r\n",
        "train_dataloader = DataLoader(\r\n",
        "    train_dataset,\r\n",
        "    sampler=RandomSampler(train_dataset),\r\n",
        "    batch_size=batch_size\r\n",
        ");\r\n",
        "\r\n",
        "val_dataloader = DataLoader(\r\n",
        "    val_dataset,\r\n",
        "    sampler=SequentialSampler(val_dataset),\r\n",
        "    batch_size=batch_size\r\n",
        ");"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIQEMz3cmazq"
      },
      "source": [
        "# get cuda device\r\n",
        "# train model\r\n",
        "# classification task"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7DGRYnfnCfm"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else print('no gpu found');"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7kQDFljmcCF"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig;"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3WHbx3jmjZz",
        "outputId": "23c66193-bd63-46eb-9d26-432dd98103b3"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\r\n",
        "    'bert-base-uncased',\r\n",
        "    num_labels=2,\r\n",
        "    output_attentions=False,\r\n",
        "    output_hidden_states=False\r\n",
        ");\r\n",
        "\r\n",
        "model.cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t2SvLTnmuI2"
      },
      "source": [
        "optimizer = AdamW(\r\n",
        "    model.parameters(),\r\n",
        "    lr=2e-5,\r\n",
        "    eps=1e-8\r\n",
        ");"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cElxEVrqnA1v"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup;"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxnZh6kbnmZN"
      },
      "source": [
        "epochs = 4;\r\n",
        "\r\n",
        "total_steps = len(train_dataloader) * epochs;\r\n",
        "\r\n",
        "scheduler = get_linear_schedule_with_warmup(\r\n",
        "    optimizer,\r\n",
        "    num_warmup_steps=0,\r\n",
        "    num_training_steps=total_steps\r\n",
        ");"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDxUr1r4nzx9"
      },
      "source": [
        "def accuracy(preds, labels):\r\n",
        "    preds = np.argmax(preds, axis=1).flatten();\r\n",
        "    labels = labels.flatten();\r\n",
        "\r\n",
        "    return np.sum(preds == labels) / len(labels);"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoFuLPtBoQkV",
        "outputId": "1d387385-c932-4187-d662-457bc65c5127"
      },
      "source": [
        "max_val_loss = np.float('inf');\r\n",
        "\r\n",
        "for e in range(epochs):\r\n",
        "    train_loss = 0;\r\n",
        "    train_acc = 0;\r\n",
        "\r\n",
        "    model.train();\r\n",
        "\r\n",
        "    for batch in train_dataloader:\r\n",
        "        b_input_ids = batch[0].to(device);\r\n",
        "        b_input_mask = batch[1].to(device);\r\n",
        "        b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "        model.zero_grad();\r\n",
        "\r\n",
        "        output = model(\r\n",
        "            b_input_ids,\r\n",
        "            token_type_ids=None,\r\n",
        "            attention_mask=b_input_mask,\r\n",
        "            labels=b_labels\r\n",
        "        );\r\n",
        "\r\n",
        "        loss = output['loss'];\r\n",
        "        preds = output['logits'].detach().cpu().numpy();\r\n",
        "        labels = b_labels.to('cpu').numpy();\r\n",
        "\r\n",
        "        train_loss += loss.item();\r\n",
        "        train_acc += accuracy(preds, labels);\r\n",
        "\r\n",
        "        loss.backward();\r\n",
        "\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0);\r\n",
        "\r\n",
        "        optimizer.step();\r\n",
        "\r\n",
        "        scheduler.step();\r\n",
        "    \r\n",
        "    avg_train_loss = train_loss / len(train_dataloader);\r\n",
        "    avg_train_acc = train_acc / len(train_dataloader);\r\n",
        "\r\n",
        "    print('average training loss for epoch: {}'.format(avg_train_loss));\r\n",
        "    print('average training accuracy for epoch: {}'.format(avg_train_acc));\r\n",
        "\r\n",
        "    # validation\r\n",
        "    val_loss = 0;\r\n",
        "    val_acc = 0;\r\n",
        "\r\n",
        "    model.eval();\r\n",
        "\r\n",
        "    for batch in val_dataloader:\r\n",
        "        b_input_ids = batch[0].to(device);\r\n",
        "        b_attention_mask = batch[1].to(device);\r\n",
        "        b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output = model(\r\n",
        "                b_input_ids,\r\n",
        "                token_type_ids=None,\r\n",
        "                attention_mask=b_attention_mask,\r\n",
        "                labels=b_labels\r\n",
        "            );\r\n",
        "        \r\n",
        "        loss = output['loss'];\r\n",
        "        preds = output['logits'].detach().cpu().numpy();\r\n",
        "        labels = b_labels.to('cpu').numpy();\r\n",
        "\r\n",
        "        val_loss += loss.item();\r\n",
        "        val_acc += accuracy(preds, labels);\r\n",
        "    \r\n",
        "    avg_val_loss = val_loss / len(val_dataloader);\r\n",
        "    avg_val_acc = val_acc / len(val_dataloader);\r\n",
        "\r\n",
        "    if avg_val_loss < max_val_loss:\r\n",
        "        max_val_loss = avg_val_loss;\r\n",
        "        torch.save(model.state_dict(), 'best_model.pt');\r\n",
        "\r\n",
        "    print('average validation loss for epoch: {}'.format(avg_val_loss));\r\n",
        "    print('average validation accuracy for epoch: {}'.format(avg_val_acc));"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training loss for epoch: 0.34845350119802687\n",
            "average training accuracy for epoch: 0.8618716931216931\n",
            "average validation loss for epoch: 0.5348166028658549\n",
            "average validation accuracy for epoch: 0.7729166666666667\n",
            "average training loss for epoch: 0.3431510994831721\n",
            "average training accuracy for epoch: 0.8634589947089948\n",
            "average validation loss for epoch: 0.5348166028658549\n",
            "average validation accuracy for epoch: 0.7729166666666667\n",
            "average training loss for epoch: 0.3471738624351996\n",
            "average training accuracy for epoch: 0.8624669312169312\n",
            "average validation loss for epoch: 0.5348166028658549\n",
            "average validation accuracy for epoch: 0.7729166666666667\n",
            "average training loss for epoch: 0.34190624875051007\n",
            "average training accuracy for epoch: 0.8639550264550264\n",
            "average validation loss for epoch: 0.5348166028658549\n",
            "average validation accuracy for epoch: 0.7729166666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "O2fZxys7pEdz",
        "outputId": "a397d522-4b72-42f3-d350-ecedfa10c960"
      },
      "source": [
        "test_df = pd.read_csv('./test.tsv', delimiter='\\t', index_col=0);\r\n",
        "\r\n",
        "test_df.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our work supported @GENE$ genetic variants as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Especially, the SNP @GENE$ and its strongly as...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Especially, the SNP rs491347 and its strongly ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The HBS1L-MYB intergenic region on chromosome ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fetal haemoglobin (@GENE$) level modifies the ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  label\n",
              "index                                                          \n",
              "0      Our work supported @GENE$ genetic variants as ...      1\n",
              "1      Especially, the SNP @GENE$ and its strongly as...      1\n",
              "2      Especially, the SNP rs491347 and its strongly ...      1\n",
              "3      The HBS1L-MYB intergenic region on chromosome ...      0\n",
              "4      Fetal haemoglobin (@GENE$) level modifies the ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzC79-G1tjRP",
        "outputId": "477a31fa-94ad-48fe-ce84-75a5d332490e"
      },
      "source": [
        "# do same preprocessing for test data\r\n",
        "test_sentences = test_df.sentence.values;\r\n",
        "test_labels = test_df.label.values;\r\n",
        "\r\n",
        "test_input_ids = [];\r\n",
        "test_attention_mask = [];\r\n",
        "\r\n",
        "for sent in test_sentences:\r\n",
        "    encoded_dict = tokenizer.encode_plus(\r\n",
        "        sent,\r\n",
        "        add_special_tokens=True,\r\n",
        "        max_length=max_len,\r\n",
        "        padding='max_length',\r\n",
        "        truncation=True,\r\n",
        "        return_attention_mask=True,\r\n",
        "        return_tensors='pt'\r\n",
        "    );\r\n",
        "\r\n",
        "    test_input_ids.append(encoded_dict['input_ids']);\r\n",
        "    test_attention_mask.append(encoded_dict['attention_mask']);\r\n",
        "\r\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0);\r\n",
        "test_attention_mask = torch.cat(test_attention_mask, dim=0);\r\n",
        "test_labels = torch.tensor(test_labels);\r\n",
        "\r\n",
        "batch_size = 32;\r\n",
        "\r\n",
        "prediction_data = TensorDataset(test_input_ids, test_attention_mask, test_labels);\r\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=SequentialSampler(prediction_data), batch_size=batch_size);\r\n",
        "\r\n",
        "print('nbr of test sentences: {}'.format(len(prediction_data)));"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nbr of test sentences: 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Oz2mEytrRm"
      },
      "source": [
        "model.load_state_dict(torch.load('best_model.pt'));\r\n",
        "\r\n",
        "model.eval();\r\n",
        "\r\n",
        "preds, real_labels = [], [];\r\n",
        "\r\n",
        "for batch in prediction_dataloader:\r\n",
        "    b_input_ids = batch[0].to(device);\r\n",
        "    b_input_mask = batch[1].to(device);\r\n",
        "    b_labels = batch[2].to(device);\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        outputs = model(\r\n",
        "            b_input_ids,\r\n",
        "            token_type_ids=None,\r\n",
        "            attention_mask=b_input_mask\r\n",
        "        );\r\n",
        "    \r\n",
        "    logits = outputs['logits'].detach().cpu().numpy();\r\n",
        "    label_ids = b_labels.to('cpu').numpy();\r\n",
        "\r\n",
        "    preds.append(logits);\r\n",
        "    real_labels.append(label_ids);"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "898bBVkGwNmK"
      },
      "source": [
        "import matplotlib.pyplot as plt;"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCXcyakzWtFI"
      },
      "source": [
        "def sigmoid(x):\r\n",
        "    return 1 / (1 + np.exp(-x));"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfRIgTnkwPkc",
        "outputId": "8138771e-adda-48cb-bfb4-a4c540b5a00c"
      },
      "source": [
        "# first batch predictions\r\n",
        "\r\n",
        "# second batch\r\n",
        "preds_1 = preds[1];\r\n",
        "preds_1 = sigmoid(preds_1);\r\n",
        "print(preds_1);\r\n",
        "preds_1 = np.argmax(preds_1, axis=1);\r\n",
        "print(preds_1);\r\n",
        "print(real_labels[1]);"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.39673707 0.6232959 ]\n",
            " [0.27848536 0.734083  ]\n",
            " [0.45340356 0.5849016 ]\n",
            " [0.30799952 0.7329959 ]\n",
            " [0.62844443 0.40633217]]\n",
            "[1 1 1 1 0]\n",
            "[1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvqm2aF2X0MW",
        "outputId": "5efe93ad-6b3f-4b37-e393-96b6e17a1488"
      },
      "source": [
        "print(accuracy(preds[1], real_labels[1]));"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "lgHCg3KPVS70",
        "outputId": "48caa998-ba75-406b-cf84-67be36e2d6bd"
      },
      "source": [
        "x = np.linspace(0, len(preds_1), len(preds_1));\r\n",
        "print(x);\r\n",
        "plt.scatter(x, preds_1, marker='.', color='b', label='predicted');\r\n",
        "plt.scatter(x, real_labels[0], color='g', label='real');\r\n",
        "plt.legend();"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.          1.03225806  2.06451613  3.09677419  4.12903226  5.16129032\n",
            "  6.19354839  7.22580645  8.25806452  9.29032258 10.32258065 11.35483871\n",
            " 12.38709677 13.41935484 14.4516129  15.48387097 16.51612903 17.5483871\n",
            " 18.58064516 19.61290323 20.64516129 21.67741935 22.70967742 23.74193548\n",
            " 24.77419355 25.80645161 26.83870968 27.87096774 28.90322581 29.93548387\n",
            " 30.96774194 32.        ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYRxIYes0XV"
      },
      "source": [
        "# Conclusion\r\n",
        "Comme il y a pas beaucoup de donnes, le modle est bias.\r\n",
        "\r\n",
        "**Travail  faire:**\r\n",
        "- Entrainer le modle sur toutes les donnes train de EU-ADR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCAQk1WKyhn3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HneqRovaMNiU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}